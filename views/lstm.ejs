<%- layout('layouts/boilerplate') %>

<h1>Simple RNN</h1>
<img src="/images/deep-learning/simple_rnn.png" alt="Elman network: Simple RNN">

<h2>State Machine Analogy</h2>
<img src="/images/deep-learning/state_machine_analogy.png" alt="State Machine Analogy for simple RNN">

<h2>RNN model</h2>
<img src="/images/deep-learning/rnn_model.png" alt="Folded and unfolded RNN">

<h2>RNN example</h2>
<img src="/images/deep-learning/example_many_to_one.png" alt="Example: Many-to-One">

<h1>Training RNNs: Backpropagation through Time (BPTT)</h1>
<p>
    Most popular loss functions:
    <ul>
        <li>regression task: Mean Squared Error</li>
        <li>classification: cross-entropy</li>
    </ul>
</p>
<p>
    Notation to explain BPTT:
    <ul>
        <li>\(\vec{d}_i\): \(i\)-th desired output, \(i \in {1,2,3}\). - given to the network.</li>
        <li> \(\vec{y}_i\): \(i\)-th calculated output, \(i \in {1,2,3}\). - extracted from the network.</li>
        <li>\(\mathcal{E}\): Expected value</li>
        <li>\(\mathcal{L}\): loss function</li>
        <li>\(E\): error</li>
    </ul>
</p>
<h2>Unfolded RNN model for first 3 timesteps</h2>
<img src="/images/deep-learning/bptt_rnn.png" alt="Unfolded RNN for the first 3 timesteps">

<h3>Update weight matrices \(W_y, W_s\) and \(W_x\) at time \(t=3\)</h3>
NOTE: \(E_3 = E_3(\vec{y}_3(W_y, \vec{s}_3(\vec{x}_3, W_x,W_s, \vec{s}_2(W_x, \vec{x}_2, W_s, \vec{s}_1(\vec{x}_1, W_x, W_s, 0)))))\).

<h4>Update output weight matrix \(W_y\)</h4>
\(\Delta W_y := \dfrac{\partial E_3}{\partial W_y} =   \dfrac{\partial E_3}{\partial \vec{y}_3} \dfrac{\partial \vec{y}_3}{\partial W_y}\)

<h4>Update state weight matrix \(W_s\)</h4>
<p>    \(\Delta W_s := \dfrac{\partial E_3}{\partial W_s} =   
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3} \dfrac{\partial \vec{s}_3}{\partial W_s} + 
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3}\dfrac{\partial \vec{s}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial W_s} +
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3}\dfrac{\partial \vec{s}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial \vec{s}_1} \dfrac{\partial \vec{s}_1}{\partial W_s}
    = \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3} \dfrac{\partial \vec{s}_3}{\partial W_s} + 
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial W_s} +
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial \vec{s}_1} \dfrac{\partial \vec{s}_1}{\partial W_s}
    \) </p>
<p>\(\Rightarrow\) <b>General equation for updating \(W_s\): </b>
    \(\dfrac{\partial E_N}{\partial W_s} = \sum\limits_{i=1}^N \dfrac{\partial E_N}{\partial \vec{y}_N}\dfrac{\partial y_N}{\partial \vec{s}_i}\dfrac{\partial \vec{s}_i}{\partial W_s}\)
</p>

<h4>Update state weight matrix \(W_x\)</h4>
<p>
    \(\Delta W_x := \dfrac{\partial E_3}{\partial W_x} =   
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3} \dfrac{\partial \vec{s}_3}{\partial W_x} + 
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3}\dfrac{\partial \vec{s}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial W_x} +
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3}\dfrac{\partial \vec{s}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial \vec{s}_1} \dfrac{\partial \vec{s}_1}{\partial W_x}
    = \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_3} \dfrac{\partial \vec{s}_3}{\partial W_x} + 
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial W_x} +
    \dfrac{\partial E_3}{\partial \vec{y}_3}\dfrac{\partial \vec{y}_3}{\partial \vec{s}_2} \dfrac{\partial \vec{s}_2}{\partial \vec{s}_1} \dfrac{\partial \vec{s}_1}{\partial W_x}
    \)
</p>
<p>\(\Rightarrow\) <b>General equation for updating \(W_x\): </b> \(\\\)
    \(\dfrac{\partial E_N}{\partial W_x} = \sum\limits_{i=1}^N \dfrac{\partial E_N}{\partial \vec{y}_N}\dfrac{\partial y_N}{\partial \vec{s}_i}\dfrac{\partial \vec{s}_i}{\partial W_x}\)
</p>

<h3>On the Vanishing Gradient Problem</h3>
<p>The formula above show that for more than a small number of timesteps a lot of multiplications will have to be performed; if the multiplied values are small, the resulting gradient weight update will be very close to 0. This is also referred to as vanishing gradient problem.</p>

<h3>Gradient Clipping</h3>
<p>
    The formula above show that for more than a small number of timesteps a lot of multiplications will have to be performed; if the multiplied values are big, the resulting gradient weight update will explode over time. To avoid an exploding gradient, so-called gradient clipping can be used, i.e.
    weight update at timestep 
\(t: \delta = 
\begin{cases}Â 
\dfrac{\partial y}{\partial W_{ij}} & \dfrac{\partial y}{\partial W_{ij}} \leq threshold \\
\dfrac{\dfrac{\partial y}{\partial W_{ij}}}{\left\lVert \dfrac{\partial y}{\partial W_{ij}} \right\rVert_2} & \dfrac{\partial y}{\partial W_{ij}} > threshold 
\end{cases}\)
</p>

<h2>Update rule for weight matrix \(U\) at timestep \(t+1\) over \(2\) timesteps</h2>
<img src="/images/deep-learning/2layer_folded_RNN.png" alt="example of two layer folded RNN model">
<p>
    \(\dfrac{\partial E_{t+1}}{\partial U} = \\
    \dfrac{\partial E_{t+1}}{\partial \vec{y}_{t+1}}\dfrac{\partial \vec{y}_{t+1}}{\partial \vec{z}_{t+1}}\dfrac{\partial \vec{z}_{t+1}}{\partial \vec{s}_{t+1}}\dfrac{\partial \vec{s}_{t+1}}{\partial U}
    + \dfrac{\partial E_{t+1}}{\partial \vec{y}_{t+1}}\dfrac{\partial \vec{y}_{t+1}}{\partial \vec{z}_{t+1}}\dfrac{\partial \vec{z}_{t+1}}{\partial \vec{z}_t}\dfrac{\partial \vec{z}_{t}}{\partial \vec{s}_{t}}\dfrac{\partial \vec{s}_{t}}{\partial U}
    + \dfrac{\partial E_{t+1}}{\partial \vec{y}_{t+1}}\dfrac{\partial \vec{y}_{t+1}}{\partial \vec{z}_{t+1}}\dfrac{\partial \vec{z}_{t+1}}{\partial \vec{s}_{t+1}}\dfrac{\partial \vec{s}_{t+1}}{\partial \vec{s}_{t}}\dfrac{\partial \vec{s}_{t}}{\partial U}. \)        
</p>

<h1>RNN neuron</h1>
<img src="/images/deep-learning/rnn_neuron.png" alt="RNN neuron">

<h1>LSTM neuron</h1>
<p>
    <img src="/images/deep-learning/lstm1.png" alt="LSTM neuron explicit version">
</p>
<p>
    Using
    \(\begin{bmatrix} STM_{t-1} & x_t \end{bmatrix}W_f + b_f = \begin{bmatrix} STM_{t-1} & x_t & 1\end{bmatrix}\begin{bmatrix} W_f \\ b_f^T \end{bmatrix} \), the diagram can be simplified to:
    <p><img src="/images/deep-learning/lstm2.png" alt="LSTM neuron more summarized version"></p>
</p>

<ul>
    In the following:
    <li><i>c</i> denotes <i>cell state</i> or <i>LTM</i> (long-term memory)</li>
    <li><i>h</i> denotes <i>hidden state</i> or <i>STM</i> (short-term memory)</li>
</ul>
A simplified and summarized schematic of the diagram above with the notation as just introduced, reads as:
<p>
    <img src="/images/deep-learning/lstm3.png" alt="LSTM summarized version">
</p>

<h2>Formulas</h2>
<p>combine factor \(\vec{N}_t = \tanh(W_n[\text{STM}_{t-1}, \vec{x}_t]+b_n) \)</p>
<p>ignore factor \(\vec{i}_t = \sigma(W_i[\text{STM}_{t-1}, \vec{x}_t]+b_i) \)</p>
<p>forget factor \(\vec{f}_t = \sigma(W_f[\text{STM}_{t-1}, \vec{x}_t]+b_f) \)</p>
<p>use factor \(\vec{U}_t = \tanh(W_u[\text{LTM}_{t-1}] \cdot \vec{f}_t+b_u) \)</p>
<p>output factor \(\vec{v}_t = \sigma(W_v[\text{STM}_{t-1}, \vec{x}_t]+b_v)\)</p>

<h2>Long Short-Term Memory with Peephole Connections</h2>
Add Long Short-Term Memory information to calculate the ignore factors, forget factors and output factors, i.e.:
<p>\(\vec{i}_t = \sigma(W_i[\text{LTM}_{t-1}, \text{STM}_{t-1}, \vec{x}_t]+b_i)\)</p>
<p>\(\vec{f}_t = \sigma(W_f[\text{LTM}_{t-1}, \text{STM}_{t-1}, \vec{x}_t]+b_f)\)</p>
<p>\(\vec{v}_t = \sigma(W_v[\text{LTM}_{t-1}, \text{STM}_{t-1}, \vec{x}_t]+b_v)\)</p>

<h2>Gated Recurrent Unit (GRU)</h2>
Combines LSTM's forget gate and learn gate into an update gate and then runs this through a combine gate.
<p>
    <img src="/images/deep-learning/GRU.png" alt="Gated Recurrent Unit">
</p>

<h2>Character-wise LSTM</h2>

In the following, \(\ell \in \{1,2\}\) will represent the layer number and \(t \in \{0, 1, \dots, M\}\) represents the time.

<h3>Definitions</h3>
<ul>
    <li> \(N\): batch size.</li>
    <li> \(M\): number of time steps</li>
    <li> \(V\): vocabulary size</li>
    <li>\(L_\ell\): number of hidden nodes in layer \(\ell\) per gate.</li>
    <li>\(W_f^\ell\): weights of <i>forget</i> gate in layer \(\ell\).</li>
    <li>\(W_n^\ell\): weights of <i>new</i> gate in layer \(\ell\).</li>
    <li>\(W_i^\ell\): weights of <i>input</i> gate in layer \(\ell\).</li>
    <li>\(W_o^\ell\): weights of <i>output</i> gate in layer \(\ell\).</li>
    <li>\(x_t\): input at time \(t\).</li>
    <li>\(h_t^\ell\): hidden state at time \(t\) in layer \(\ell\) - short-term memory.</li>
    <li> \(c_t^\ell\): cell state at time \(t\) in layer \(\ell\) - long-term memory.</li>
    <li> \(\mathbb{1}_N\): \(N \times N\)-matrix filled with all ones.</li>
</ul>

Inferred measures:
<ul>
    <li>number of characters per batch = \(N \cdot M\)</li>
    <li>hidden state tensor output size in layer \(k = N \cdot M \cdot L_k\) </li>
</ul>

Important note:
<ul>
    <li>Tensorflow LSTM cell corresponds to (termed correctly) an LSTM layer.</li>
</ul>

<h3>Process</h3>
<p>
    <b src="/images/deep-learning/charlstmprocess_1.png" alt="Characterwise LSTM 2layer full part 1">
    <img src="/images/deep-learning/charlstmprocess_2.png" alt="Characterwise LSTM 2layer full part 2">
</p>
